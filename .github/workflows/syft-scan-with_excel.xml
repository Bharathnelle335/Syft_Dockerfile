name: Syft scan Dockerfiles (static)

on:
  workflow_dispatch:
    inputs:
      repo_url:
        description: "Git repository URL containing Dockerfile(s)"
        required: true
        type: string
      ref:
        description: "Branch, tag, or commit to checkout in target repo"
        required: false
        default: "main"
        type: string
      dockerfile_glob:
        description: "Glob to find Dockerfiles (e.g. **/Dockerfile or **/Dockerfile*)"
        required: false
        default: "**/Dockerfile*"
        type: string
      output_format:
        description: "SBOM output format"
        required: false
        default: "json"
        type: choice
        options: [json, spdx-json, cyclonedx-json]

permissions:
  contents: read

jobs:
  discover:
    name: Discover Dockerfiles
    runs-on: ubuntu-latest
    outputs:
      files_json: ${{ steps.collect.outputs.files_json }}
      has_files: ${{ steps.collect.outputs.has_files }}
    steps:
      - name: Clone target repo
        run: |
          set -euo pipefail
          git clone --depth 1 "${{ inputs.repo_url }}" target
          cd target
          git fetch --depth 1 origin "${{ inputs.ref }}" || true
          git checkout "${{ inputs.ref }}" || true
          echo "Checked out $(git rev-parse --short HEAD)"

      - name: Find Dockerfiles
        id: collect
        shell: bash
        env:
          GLOB: ${{ inputs.dockerfile_glob }}
        run: |
          set -euo pipefail
          cd target
          FILES_JSON=$(python3 -c 'import json,glob,os; p=os.getenv("GLOB","**/Dockerfile*"); f=[x for x in glob.glob(p,recursive=True) if os.path.isfile(x)]; print("[]") if not f else print(json.dumps(f))')
          if [ "$FILES_JSON" = "[]" ]; then
            echo "files_json=[]" >> "$GITHUB_OUTPUT"
            echo "has_files=false" >> "$GITHUB_OUTPUT"
          else
            echo "files_json=$FILES_JSON" >> "$GITHUB_OUTPUT"
            echo "has_files=true" >> "$GITHUB_OUTPUT"
          fi

  scan:
    name: Syft scan (${{ matrix.file }})
    needs: discover
    if: needs.discover.outputs.has_files == 'true'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        file: ${{ fromJson(needs.discover.outputs.files_json) }}
    steps:
      - name: Clone target repo
        run: |
          set -euo pipefail
          git clone --depth 1 "${{ inputs.repo_url }}" target
          cd target
          git fetch --depth 1 origin "${{ inputs.ref }}" || true
          git checkout "${{ inputs.ref }}" || true

      - name: Install Syft
        run: |
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
          syft version

      - name: Scan Dockerfile (static)
        id: scan_step
        working-directory: target
        env:
          OUT_FMT: ${{ inputs.output_format }}
          FILE: ${{ matrix.file }}
        run: |
          set -euo pipefail
          mkdir -p ../sboms
          SAFE_NAME=$(echo "$FILE" | tr '/\\ ' '___' | tr -cd '[:alnum:]_.-')
          OUT="../sboms/syft-$SAFE_NAME.${OUT_FMT/+-/_}.json"
          echo "Scanning $FILE -> $OUT"
          syft "$FILE" -o "$OUT_FMT" > "$OUT"
          echo "safe_name=$SAFE_NAME" >> "$GITHUB_OUTPUT"

      - name: Upload SBOM (per Dockerfile)
        uses: actions/upload-artifact@v4
        with:
          name: sbom-${{ steps.scan_step.outputs.safe_name }}
          path: sboms/*.json
          if-no-files-found: error

  combine:
    name: Combine & Export
    needs: [discover, scan]
    if: needs.discover.outputs.has_files == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Download all SBOM artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: sbom-*
          merge-multiple: true
          path: sboms

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Combine JSON SBOMs
        run: |
          set -euo pipefail
          mkdir -p combined
          jq -s '.' sboms/*.json > combined/syft-combined.json
          echo "Wrote combined/syft-combined.json"

      - name: Install Python deps (pandas/openpyxl)
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install pandas openpyxl

      - name: Convert SBOM JSONs to Excel
        run: |
          set -euo pipefail
          python3 <<'PY'
import json, os, glob
from pathlib import Path
import pandas as pd

SBOMS_DIR = Path("sboms")
COMBINED = Path("combined/syft-combined.json")
OUT = Path("combined/syft-sboms.xlsx")

def join_list(x):
    if x is None: return ""
    if isinstance(x, list):
        parts=[]
        for it in x:
            if isinstance(it, (dict,list)):
                parts.append(json.dumps(it, ensure_ascii=False, sort_keys=True))
            else:
                parts.append(str(it))
        return "; ".join(parts)
    return str(x)

def flatten_dict(d, parent_key="", sep="."):
    items = []
    for k, v in (d or {}).items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            items.append((new_key, join_list(v)))
        else:
            items.append((new_key, v))
    return dict(items)

def syft_to_dfs(obj, source_label):
    packages_rows, rel_rows, summary_rows = [], [], []

    for art in obj.get("artifacts", []):
        row = {
            "source": source_label,
            "id": art.get("id"),
            "name": art.get("name"),
            "version": art.get("version"),
            "type": art.get("type"),
            "foundBy": art.get("foundBy"),
            "language": art.get("language"),
            "purl": art.get("purl"),
            "cpe": "; ".join(art.get("cpes", []) or []),
            "licenses": "; ".join(
                [lic.get("value","") if isinstance(lic, dict) else str(lic)
                 for lic in (art.get("licenses") or [])]
            ),
            "locations": "; ".join([loc.get("path","") for loc in (art.get("locations") or [])]),
            "metadataType": art.get("metadataType"),
        }
        md = art.get("metadata")
        if isinstance(md, dict):
            row.update({f"metadata.{k}": v for k, v in flatten_dict(md).items()})
        packages_rows.append(row)

    for rel in obj.get("artifactRelationships", []):
        rel_rows.append({
            "source": source_label,
            "parent": rel.get("parent"),
            "child": rel.get("child"),
            "type": rel.get("type"),
            "metadata": json.dumps(rel.get("metadata", {}), ensure_ascii=False, sort_keys=True)
        })

    desc = obj.get("descriptor", {}) or {}
    distro = obj.get("distro", {}) or {}
    src = obj.get("source", {}) or {}
    summary_rows.append({
        "source": source_label,
        "descriptor.name": desc.get("name"),
        "descriptor.version": desc.get("version"),
        "distro.name": distro.get("name"),
        "distro.version": distro.get("version"),
        "src.type": src.get("type"),
        "src.target": json.dumps(src.get("target"), ensure_ascii=False, sort_keys=True)
                      if isinstance(src.get("target"), (dict,list)) else src.get("target"),
    })

    df_packages = pd.DataFrame(packages_rows or [{}]).dropna(axis=1, how="all")
    df_rels = pd.DataFrame(rel_rows or [{}]).dropna(axis=1, how="all")
    df_summary = pd.DataFrame(summary_rows or [{}]).dropna(axis=1, how="all")
    return df_packages, df_rels, df_summary

files = sorted([*SBOMS_DIR.glob("*.json")])
if COMBINED.exists():
    files.append(COMBINED)

all_packages, all_rels, all_summaries = [], [], []

for f in files:
    label = f.as_posix()
    with open(f, "r", encoding="utf-8") as fh:
        data = json.load(fh)
    objs = data if isinstance(data, list) else [data]
    for idx, obj in enumerate(objs):
        source_label = f"{label}#{idx}" if len(objs) > 1 else label
        if isinstance(obj, dict) and ("artifacts" in obj or "artifactRelationships" in obj):
            p, r, s = syft_to_dfs(obj, source_label)
        else:
            flat = flatten_dict(obj)
            import pandas as pd
            p = pd.DataFrame([{**flat, "source": source_label}])
            r = pd.DataFrame([{"source": source_label}])
            s = pd.DataFrame([{"source": source_label}])
        all_packages.append(p)
        all_rels.append(r)
        all_summaries.append(s)

df_packages = pd.concat(all_packages, ignore_index=True) if all_packages else pd.DataFrame()
df_rels = pd.concat(all_rels, ignore_index=True) if all_rels else pd.DataFrame()
df_summary = pd.concat(all_summaries, ignore_index=True) if all_summaries else pd.DataFrame()

front_cols = [c for c in ["source","name","version","type","language","purl","licenses","cpe","locations","metadataType","foundBy","id"] if c in df_packages.columns]
other_cols = [c for c in df_packages.columns if c not in front_cols]
df_packages = df_packages[front_cols + other_cols] if not df_packages.empty else df_packages

OUT.parent.mkdir(parents=True, exist_ok=True)
with pd.ExcelWriter(OUT, engine="openpyxl") as xw:
    (df_packages if not df_packages.empty else pd.DataFrame([{"note":"no packages"}])).to_excel(xw, index=False, sheet_name="packages")
    (df_rels if not df_rels.empty else pd.DataFrame([{"note":"no relationships"}])).to_excel(xw, index=False, sheet_name="relationships")
    (df_summary if not df_summary.empty else pd.DataFrame([{"note":"no summary"}])).to_excel(xw, index=False, sheet_name="summary")

print(f"Wrote {OUT}")
PY

      - name: Upload combined JSON
        uses: actions/upload-artifact@v4
        with:
          name: sbom-combined
          path: combined/syft-combined.json

      - name: Upload Excel
        uses: actions/upload-artifact@v4
        with:
          name: sbom-excel
          path: combined/syft-sboms.xlsx

  no_files_hint:
    name: No Dockerfiles found
    needs: discover
    if: needs.discover.outputs.has_files != 'true'
    runs-on: ubuntu-latest
    steps:
      - run: echo "No Dockerfiles matched the pattern. Adjust 'dockerfile_glob' and re-run."
