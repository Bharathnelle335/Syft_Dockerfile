name: Syft scan Dockerfiles (static, no pandas)

on:
  workflow_dispatch:
    inputs:
      repo_url:
        description: "Git repository URL containing Dockerfile(s)"
        required: true
        type: string
      ref:
        description: "Branch, tag, or commit to checkout in target repo"
        required: false
        default: "main"
        type: string
      dockerfile_glob:
        description: "Glob to find Dockerfiles (e.g. **/Dockerfile or **/Dockerfile*)"
        required: false
        default: "**/Dockerfile*"
        type: string
      output_format:
        description: "SBOM output format"
        required: false
        default: "json"
        type: choice
        options: [json, spdx-json, cyclonedx-json]

permissions:
  contents: read

jobs:
  discover:
    name: Discover Dockerfiles
    runs-on: ubuntu-latest
    outputs:
      files_json: ${{ steps.collect.outputs.files_json }}
      has_files: ${{ steps.collect.outputs.has_files }}
    steps:
      - name: Clone target repo
        run: |
          set -euo pipefail
          git clone --depth 1 "${{ inputs.repo_url }}" target
          cd target
          git fetch --depth 1 origin "${{ inputs.ref }}" || true
          git checkout "${{ inputs.ref }}" || true
          echo "Checked out $(git rev-parse --short HEAD)"

      - name: Find Dockerfiles
        id: collect
        shell: bash
        env:
          GLOB: ${{ inputs.dockerfile_glob }}
        run: |
          set -euo pipefail
          cd target
          FILES_JSON=$(python3 -c 'import json,glob,os; p=os.getenv("GLOB","**/Dockerfile*"); f=[x for x in glob.glob(p,recursive=True) if os.path.isfile(x)]; print("[]") if not f else print(json.dumps(f))')
          if [ "$FILES_JSON" = "[]" ]; then
            echo "files_json=[]" >> "$GITHUB_OUTPUT"
            echo "has_files=false" >> "$GITHUB_OUTPUT"
          else
            echo "files_json=$FILES_JSON" >> "$GITHUB_OUTPUT"
            echo "has_files=true" >> "$GITHUB_OUTPUT"
          fi

  scan:
    name: Syft scan (${{ matrix.file }})
    needs: discover
    if: needs.discover.outputs.has_files == 'true'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        file: ${{ fromJson(needs.discover.outputs.files_json) }}
    steps:
      - name: Clone target repo
        run: |
          set -euo pipefail
          git clone --depth 1 "${{ inputs.repo_url }}" target
          cd target
          git fetch --depth 1 origin "${{ inputs.ref }}" || true
          git checkout "${{ inputs.ref }}" || true

      - name: Install Syft
        run: |
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
          syft version

      - name: Scan Dockerfile (static)
        id: scan_step
        working-directory: target
        env:
          OUT_FMT: ${{ inputs.output_format }}
          FILE: ${{ matrix.file }}
        run: |
          set -euo pipefail
          mkdir -p ../sboms
          SAFE_NAME=$(echo "$FILE" | tr '/\\ ' '___' | tr -cd '[:alnum:]_.-')
          OUT="../sboms/syft-$SAFE_NAME.${OUT_FMT/+-/_}.json"
          echo "Scanning $FILE -> $OUT"
          syft "$FILE" -o "$OUT_FMT" > "$OUT"
          echo "safe_name=$SAFE_NAME" >> "$GITHUB_OUTPUT"

      - name: Upload SBOM (per Dockerfile)
        uses: actions/upload-artifact@v4
        with:
          name: sbom-${{ steps.scan_step.outputs.safe_name }}
          path: sboms/*.json
          if-no-files-found: error

  combine:
    name: Combine & Export CSV
    needs: [discover, scan]
    if: needs.discover.outputs.has_files == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Download all SBOM artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: sbom-*
          merge-multiple: true
          path: sboms

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Combine JSON SBOMs
        run: |
          set -euo pipefail
          mkdir -p combined
          jq -s '.' sboms/*.json > combined/syft-combined.json
          echo "Wrote combined/syft-combined.json"

      - name: Convert SBOM JSONs to CSV (Excel-compatible)
        run: |
          set -euo pipefail
          python3 <<'PY'
import json, csv, os, glob
from pathlib import Path

SBOMS_DIR = Path("sboms")
COMBINED = Path("combined/syft-combined.json")
OUT_DIR = Path("combined/csv")
OUT_DIR.mkdir(parents=True, exist_ok=True)

def flatten_dict(d, parent_key="", sep="."):
    items = []
    for k, v in (d or {}).items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            items.append((new_key, "; ".join(map(str, v))))
        else:
            items.append((new_key, v))
    return dict(items)

def write_csv(rows, file_path):
    if not rows:
        with open(file_path, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow(["note"])
            csv.writer(f).writerow(["no data"])
        return
    keys = sorted({k for r in rows for k in r.keys()})
    with open(file_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=keys)
        writer.writeheader()
        for r in rows:
            writer.writerow(r)

def parse_syft_json(data, label):
    if not isinstance(data, dict):
        return [], [], []
    packages, rels, summaries = [], [], []
    for art in data.get("artifacts", []):
        base = {"source": label}
        base.update(flatten_dict(art))
        packages.append(base)
    for rel in data.get("artifactRelationships", []):
        rels.append({"source": label, **flatten_dict(rel)})
    summaries.append({"source": label, **flatten_dict({
        "descriptor": data.get("descriptor"),
        "distro": data.get("distro"),
        "source_meta": data.get("source")
    })})
    return packages, rels, summaries

# Gather JSON files
json_files = sorted(SBOMS_DIR.glob("*.json"))
if COMBINED.exists():
    json_files.append(COMBINED)

all_packages, all_rels, all_summaries = [], [], []

for f in json_files:
    label = f.as_posix()
    with open(f, "r", encoding="utf-8") as fh:
        content = json.load(fh)
    objs = content if isinstance(content, list) else [content]
    for i, obj in enumerate(objs):
        src_label = f"{label}#{i}" if len(objs) > 1 else label
        p, r, s = parse_syft_json(obj, src_label)
        all_packages.extend(p)
        all_rels.extend(r)
        all_summaries.extend(s)

write_csv(all_packages, OUT_DIR / "packages.csv")
write_csv(all_rels, OUT_DIR / "relationships.csv")
write_csv(all_summaries, OUT_DIR / "summary.csv")
print(f"Wrote CSVs to {OUT_DIR}")
PY

      - name: Upload combined outputs
        uses: actions/upload-artifact@v4
        with:
          name: sbom-csv
          path: combined/csv/

  no_files_hint:
    name: No Dockerfiles found
    needs: discover
    if: needs.discover.outputs.has_files != 'true'
    runs-on: ubuntu-latest
    steps:
      - run: echo "No Dockerfiles matched the pattern. Adjust 'dockerfile_glob' and re-run."
