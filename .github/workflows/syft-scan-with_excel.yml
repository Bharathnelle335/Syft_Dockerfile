name: Syft scan Dockerfiles (static, pure-jq CSV)

on:
  workflow_dispatch:
    inputs:
      repo_url:
        description: "Git repository URL containing Dockerfile(s)"
        required: true
        type: string
      ref:
        description: "Branch, tag, or commit to checkout in target repo"
        required: false
        default: "main"
        type: string
      dockerfile_glob:
        description: "Glob to find Dockerfiles (e.g. **/Dockerfile or **/Dockerfile*)"
        required: false
        default: "**/Dockerfile*"
        type: string
      output_format:
        description: "SBOM output format"
        required: false
        default: "json"
        type: choice
        options:
          - json
          - spdx-json
          - cyclonedx-json

permissions:
  contents: read

jobs:
  discover:
    name: Discover Dockerfiles
    runs-on: ubuntu-latest
    outputs:
      files_json: ${{ steps.collect.outputs.files_json }}
      has_files: ${{ steps.collect.outputs.has_files }}
    steps:
      - name: Clone target repo
        run: |
          set -euo pipefail
          git clone --depth 1 "${{ inputs.repo_url }}" target
          cd target
          git fetch --depth 1 origin "${{ inputs.ref }}" || true
          git checkout "${{ inputs.ref }}" || true
          echo "Checked out $(git rev-parse --short HEAD)"

      - name: Find Dockerfiles
        id: collect
        shell: bash
        env:
          GLOB: ${{ inputs.dockerfile_glob }}
        run: |
          set -euo pipefail
          cd target
          FILES_JSON=$(python3 -c 'import json,glob,os; p=os.getenv("GLOB","**/Dockerfile*"); f=[x for x in glob.glob(p,recursive=True) if os.path.isfile(x)]; print("[]") if not f else print(json.dumps(f))')
          if [ "$FILES_JSON" = "[]" ]; then
            echo "files_json=[]" >> "$GITHUB_OUTPUT"
            echo "has_files=false" >> "$GITHUB_OUTPUT"
          else
            echo "files_json=$FILES_JSON" >> "$GITHUB_OUTPUT"
            echo "has_files=true" >> "$GITHUB_OUTPUT"
          fi

  scan:
    name: Syft scan (${{ matrix.file }})
    needs: discover
    if: needs.discover.outputs.has_files == 'true'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        file: ${{ fromJson(needs.discover.outputs.files_json) }}
    steps:
      - name: Clone target repo
        run: |
          set -euo pipefail
          git clone --depth 1 "${{ inputs.repo_url }}" target
          cd target
          git fetch --depth 1 origin "${{ inputs.ref }}" || true
          git checkout "${{ inputs.ref }}" || true

      - name: Install Syft
        run: |
          set -euo pipefail
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
          syft version

      - name: Scan Dockerfile (static)
        id: scan_step
        working-directory: target
        env:
          OUT_FMT: ${{ inputs.output_format }}
          FILE: ${{ matrix.file }}
        run: |
          set -euo pipefail
          mkdir -p ../sboms
          SAFE_NAME=$(echo "$FILE" | tr '/\\ ' '___' | tr -cd '[:alnum:]_.-')
          OUT="../sboms/syft-$SAFE_NAME.${OUT_FMT/+-/_}.json"
          echo "Scanning $FILE -> $OUT"
          syft "$FILE" -o "$OUT_FMT" > "$OUT"
          echo "safe_name=$SAFE_NAME" >> "$GITHUB_OUTPUT"

      - name: Upload SBOM (per Dockerfile)
        uses: actions/upload-artifact@v4
        with:
          name: sbom-${{ steps.scan_step.outputs.safe_name }}
          path: sboms/*.json
          if-no-files-found: error

  combine:
    name: Combine & Export CSV (jq only)
    needs: [discover, scan]
    if: needs.discover.outputs.has_files == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Download all SBOM artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: sbom-*
          merge-multiple: true
          path: sboms

      - name: Install jq
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Combine JSON SBOMs
        run: |
          set -euo pipefail
          mkdir -p combined
          jq -s '.' sboms/*.json > combined/syft-combined.json
          echo "Wrote combined/syft-combined.json"

      - name: Prepare jq programs (no heredocs)
        run: |
          set -euo pipefail
          mkdir -p combined/jq combined/csv
          echo 'def flatten: reduce paths(scalars) as $p ({}; . + { ($p|map(tostring)|join(".")) : getpath($p) });' > combined/jq/flatten.jq
          # packages.jq
          {
            echo 'include "flatten";'
            echo '[ inputs'
            echo '  | ( (type=="array")? .[] : . ) as $doc'
            echo '  | ($doc.artifacts // [])[]'
            echo '  | flatten'
            echo '  | .source = ( $doc.__src // "unknown" )'
            echo '] as $rows'
            echo '| ( ($rows | reduce .[] as $r ({}; . + $r) | keys) ) as $H'
            echo '| ($H | @csv),'
            echo '  ( $rows[] | [ $H[] as $k | ( .[$k] // "" ) ] | @csv )'
          } > combined/jq/packages.jq
          # relationships.jq
          {
            echo 'include "flatten";'
            echo '[ inputs'
            echo '  | ( (type=="array")? .[] : . ) as $doc'
            echo '  | ($doc.artifactRelationships // [])'
            echo '  | map( flatten + {source: ($doc.__src // "unknown")} )'
            echo '  | .[]'
            echo '] as $rows'
            echo '| ( ($rows | reduce .[] as $r ({}; . + $r) | keys) ) as $H'
            echo '| ($H | @csv),'
            echo '  ( $rows[] | [ $H[] as $k | ( .[$k] // "" ) ] | @csv )'
          } > combined/jq/relationships.jq
          # summary.jq
          {
            echo 'include "flatten";'
            echo '[ inputs'
            echo '  | ( (type=="array")? .[] : . ) as $doc'
            echo '  | {'
            echo '      source: ($doc.__src // "unknown"),'
            echo '      descriptor: $doc.descriptor,'
            echo '      distro: $doc.distro,'
            echo '      source_meta: $doc.source'
            echo '    }'
            echo '  | flatten'
            echo '] as $rows'
            echo '| ( ($rows | reduce .[] as $r ({}; . + $r) | keys) ) as $H'
            echo '| ($H | @csv),'
            echo '  ( $rows[] | [ $H[] as $k | ( .[$k] // "" ) ] | @csv )'
          } > combined/jq/summary.jq

      - name: Export packages.csv
        shell: bash
        run: |
          set -euo pipefail
          jq -r -L combined/jq -f combined/jq/packages.jq \
            <(for f in sboms/*.json; do jq --arg __src "$f" '(. + { "__src":$__src })' "$f"; done) \
            combined/syft-combined.json \
            > combined/csv/packages.csv

      - name: Export relationships.csv
        shell: bash
        run: |
          set -euo pipefail
          jq -r -L combined/jq -f combined/jq/relationships.jq \
            <(for f in sboms/*.json; do jq --arg __src "$f" '(. + { "__src":$__src })' "$f"; done) \
            combined/syft-combined.json \
            > combined/csv/relationships.csv

      - name: Export summary.csv
        shell: bash
        run: |
          set -euo pipefail
          jq -r -L combined/jq -f combined/jq/summary.jq \
            <(for f in sboms/*.json; do jq --arg __src "$f" '(. + { "__src":$__src })' "$f"; done) \
            combined/syft-combined.json \
            > combined/csv/summary.csv

      - name: Upload combined JSON
        uses: actions/upload-artifact@v4
        with:
          name: sbom-combined
          path: combined/syft-combined.json

      - name: Upload CSVs
        uses: actions/upload-artifact@v4
        with:
          name: sbom-csv
          path: combined/csv/

  no_files_hint:
    name: No Dockerfiles found
    needs: discover
    if: needs.discover.outputs.has_files != 'true'
    runs-on: ubuntu-latest
    steps:
      - run: echo "No Dockerfiles matched the pattern. Adjust 'dockerfile_glob' and re-run."
