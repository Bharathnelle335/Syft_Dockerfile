name: Syft scan Dockerfiles (static, pure-jq CSV)

on:
  workflow_dispatch:
    inputs:
      repo_url:
        description: "Git repository URL containing Dockerfile(s)"
        required: true
        type: string
      ref:
        description: "Branch, tag, or commit to checkout in target repo"
        required: false
        default: "main"
        type: string
      dockerfile_glob:
        description: "Glob to find Dockerfiles (e.g. **/Dockerfile or **/Dockerfile*)"
        required: false
        default: "**/Dockerfile*"
        type: string
      output_format:
        description: "SBOM output format"
        required: false
        default: "json"
        type: choice
        options:
          - json
          - spdx-json
          - cyclonedx-json

permissions:
  contents: read

jobs:
  discover:
    name: Discover Dockerfiles
    runs-on: ubuntu-latest
    outputs:
      files_json: ${{ steps.collect.outputs.files_json }}
      has_files: ${{ steps.collect.outputs.has_files }}
    steps:
      - name: Clone target repo
        run: |
          set -euo pipefail
          git clone --depth 1 "${{ inputs.repo_url }}" target
          cd target
          git fetch --depth 1 origin "${{ inputs.ref }}" || true
          git checkout "${{ inputs.ref }}" || true
          echo "Checked out $(git rev-parse --short HEAD)"

      - name: Find Dockerfiles
        id: collect
        shell: bash
        env:
          GLOB: ${{ inputs.dockerfile_glob }}
        run: |
          set -euo pipefail
          cd target
          FILES_JSON=$(python3 -c 'import json,glob,os; p=os.getenv("GLOB","**/Dockerfile*"); f=[x for x in glob.glob(p,recursive=True) if os.path.isfile(x)]; print("[]") if not f else print(json.dumps(f))')
          if [ "$FILES_JSON" = "[]" ]; then
            echo "files_json=[]" >> "$GITHUB_OUTPUT"
            echo "has_files=false" >> "$GITHUB_OUTPUT"
          else
            echo "files_json=$FILES_JSON" >> "$GITHUB_OUTPUT"
            echo "has_files=true" >> "$GITHUB_OUTPUT"
          fi

  scan:
    name: Syft scan (${{ matrix.file }})
    needs: discover
    if: needs.discover.outputs.has_files == 'true'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        file: ${{ fromJson(needs.discover.outputs.files_json) }}
    steps:
      - name: Clone target repo
        run: |
          set -euo pipefail
          git clone --depth 1 "${{ inputs.repo_url }}" target
          cd target
          git fetch --depth 1 origin "${{ inputs.ref }}" || true
          git checkout "${{ inputs.ref }}" || true

      - name: Install Syft
        run: |
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
          syft version

      - name: Scan Dockerfile (static)
        id: scan_step
        working-directory: target
        env:
          OUT_FMT: ${{ inputs.output_format }}
          FILE: ${{ matrix.file }}
        run: |
          set -euo pipefail
          mkdir -p ../sboms
          SAFE_NAME=$(echo "$FILE" | tr '/\\ ' '___' | tr -cd '[:alnum:]_.-')
          OUT="../sboms/syft-$SAFE_NAME.${OUT_FMT/+-/_}.json"
          echo "Scanning $FILE -> $OUT"
          syft "$FILE" -o "$OUT_FMT" > "$OUT"
          echo "safe_name=$SAFE_NAME" >> "$GITHUB_OUTPUT"

      - name: Upload SBOM (per Dockerfile)
        uses: actions/upload-artifact@v4
        with:
          name: sbom-${{ steps.scan_step.outputs.safe_name }}
          path: sboms/*.json
          if-no-files-found: error

  combine:
    name: Combine & Export CSV (pure jq)
    needs: [discover, scan]
    if: needs.discover.outputs.has_files == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Download all SBOM artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: sbom-*
          merge-multiple: true
          path: sboms

      - name: Install jq
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Combine JSON SBOMs
        run: |
          set -euo pipefail
          mkdir -p combined
          jq -s '.' sboms/*.json > combined/syft-combined.json
          echo "Wrote combined/syft-combined.json"

      - name: Export packages.csv (flattened)
        run: |
          set -euo pipefail
          mkdir -p combined/csv
          # Build a single rows array from ALL per-file JSONs and the combined file (dedup if same docs)
          jq -r '
            def flatten:
              reduce (paths(scalars) as $p ({}; . + { ($p|map(tostring)|join(".")) : getpath($p) }));
            # Gather rows from individual files:
            [ inputs
              | ( (type=="array")? .[] : . ) as $doc
              | ($doc.artifacts // [])[]
              | flatten
              | .source = ( $doc.__src // "unknown" )
            ] as $rows
            # CSV header = union of keys
            | ( ($rows | reduce .[] as $r ({}; . + $r) | keys) ) as $H
            | ($H | @csv),
              ( $rows[] | [ $H[] as $k | ( .[$k] // "" ) ] | @csv )
          ' \
          <(for f in sboms/*.json; do jq --arg __src "$f" '(. + { "__src":$__src })' "$f"; done) \
          combined/syft-combined.json \
          > combined/csv/packages.csv

      - name: Export relationships.csv (flattened)
        run: |
          set -euo pipefail
          mkdir -p combined/csv
          jq -r '
            def flatten:
              reduce (paths(scalars) as $p ({}; . + { ($p|map(tostring)|join(".")) : getpath($p) }));
            [ inputs
              | ( (type=="array")? .[] : . ) as $doc
              | ($doc.artifactRelationships // [])
              | map( flatten + {source: ($doc.__src // "unknown")} )
              | .[]
            ] as $rows
            | ( ($rows | reduce .[] as $r ({}; . + $r) | keys) ) as $H
            | ($H | @csv),
              ( $rows[] | [ $H[] as $k | ( .[$k] // "" ) ] | @csv )
          ' \
          <(for f in sboms/*.json; do jq --arg __src "$f" '(. + { "__src":$__src })' "$f"; done) \
          combined/syft-combined.json \
          > combined/csv/relationships.csv

      - name: Export summary.csv (descriptor/distro/source)
        run: |
          set -euo pipefail
          mkdir -p combined/csv
          jq -r '
            def flatten:
              reduce (paths(scalars) as $p ({}; . + { ($p|map(tostring)|join(".")) : getpath($p) }));
            [ inputs
              | ( (type=="array")? .[] : . ) as $doc
              | {
                  source: ($doc.__src // "unknown"),
                  descriptor: $doc.descriptor,
                  distro: $doc.distro,
                  source_meta: $doc.source
                }
              | flatten
            ] as $rows
            | ( ($rows | reduce .[] as $r ({}; . + $r) | keys) ) as $H
            | ($H | @csv),
              ( $rows[] | [ $H[] as $k | ( .[$k] // "" ) ] | @csv )
          ' \
          <(for f in sboms/*.json; do jq --arg __src "$f" '(. + { "__src":$__src })' "$f"; done) \
          combined/syft-combined.json \
          > combined/csv/summary.csv

      - name: Upload CSVs
        uses: actions/upload-artifact@v4
        with:
          name: sbom-csv
          path: combined/csv/

  no_files_hint:
    name: No Dockerfiles found
    needs: discover
    if: needs.discover.outputs.has_files != 'true'
    runs-on: ubuntu-latest
    steps:
      - run: echo "No Dockerfiles matched the pattern. Adjust 'dockerfile_glob' and re-run."
